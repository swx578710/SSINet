{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3367e635-b5d7-42d1-8c0d-170f23c6c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, multiply, Input, Dropout, Conv1D, Flatten, GRU, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Add, concatenate, GaussianNoise, Layer, BatchNormalization, Reshape\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "from tensorflow.keras.activations import relu, elu, tanh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from esinet import util\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from esinet import Simulation, Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a25b71-8569-4482-9e34-3781680726cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention_module(input_tensor, reduction_ratio=8):\n",
    "    # 获取输入通道数\n",
    "    channels = input_tensor.shape[-1]\n",
    "    \n",
    "    # 压缩阶段\n",
    "    squeeze = GlobalAveragePooling1D()(input_tensor)\n",
    "    \n",
    "    # 激励阶段\n",
    "    excitation = Dense(channels // reduction_ratio, activation='relu')(squeeze)\n",
    "    excitation = Dense(channels, activation='sigmoid')(excitation)\n",
    "    \n",
    "    # 将激励层的输出调整形状以匹配原始输入的形状\n",
    "    excitation = Reshape((1, channels))(excitation)\n",
    "    \n",
    "    # 重标定阶段，逐元素乘法\n",
    "    scale = multiply([input_tensor, excitation],  name=\"multiply2\")\n",
    "    \n",
    "    \n",
    "    return scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c76e4992-ebe0-4619-807f-aed6ec191c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=0)(x, x)\n",
    "    x = Add()([x, inputs])\n",
    "\n",
    "    # Feed Forward Part\n",
    "    ff = LayerNormalization(epsilon=1e-6)(x)\n",
    "    ff = TimeDistributed(Dense(ff_dim, activation=\"relu\"))(ff)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = TimeDistributed(Dense(inputs.shape[-1]))(ff)\n",
    "    ff = Add()([ff, x])\n",
    "\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23856cb-f2fc-4148-9c1c-e8dc38ababf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define several different loss functions\n",
    "def combined_loss_transform(V):\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # MSE Loss\n",
    "        huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "        \n",
    "        # Cosine Similarity Loss\n",
    "        # 使用 tf.keras.losses.cosine_similarity，并确保结果为正值\n",
    "        cosine_loss = 1+tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    \n",
    "        transformed = tf.matmul(y_pred, V)\n",
    "        l1_loss = tf.reduce_mean(tf.abs(transformed))\n",
    "        \n",
    "        # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "        combined = 10000 * huber + 10*cosine_loss + 100*l1_loss\n",
    "        \n",
    "        return combined\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def combined_loss_Laplactransform(L):\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # MSE Loss\n",
    "        huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "        \n",
    "        # Cosine Similarity Loss\n",
    "        # 使用 tf.keras.losses.cosine_similarity，并确保结果为正值\n",
    "        cosine_loss = 1+tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    \n",
    "        # 计算拉普拉斯正则化项\n",
    "        # y_pred需要与拉普拉斯矩阵的维度匹配\n",
    "        laplacian_reg = tf.matmul(tf.matmul(y_pred, L), y_pred, transpose_a=True)\n",
    "        l1_loss = tf.reduce_mean(tf.abs(laplacian_reg))\n",
    "        \n",
    "        # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "        combined = 10000 * huber + 3*cosine_loss + 100*l1_loss\n",
    "        \n",
    "        return combined\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def wight_combined_loss(y_true, y_pred):\n",
    "\n",
    "    # 创建一个掩码，标记非零值\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    # 使用掩码过滤出非零元素\n",
    "    y_true_filtered = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_filtered = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "    # MSE Loss\n",
    "    huber = tf.keras.losses.Huber(delta=0.5)(y_true, y_pred)\n",
    "    wMSE = tf.keras.losses.MeanSquaredError()(y_true_filtered, y_pred_filtered)\n",
    "    \n",
    "    # Cosine Similarity Loss\n",
    "    # 使用 tf.keras.losses.cosine_similarity，并确保结果为正值\n",
    "    cosine_loss = 1+tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    \n",
    "    # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "    #combined = 1000 * huber + cosine_loss + wMSE\n",
    "    combined = 1000 * huber + cosine_loss\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def wight_combined_loss2(y_true, y_pred):\n",
    "    \n",
    "    # 设定权重\n",
    "    weights = tf.where(tf.not_equal(y_true, 0), 1.0, 0)\n",
    "    # 计算加权MSE\n",
    "    wmse=tf.reduce_mean(weights * tf.square(y_true - y_pred))\n",
    "\n",
    "    # MSE Loss\n",
    "    huber = tf.keras.losses.Huber(delta=0.5)(y_true, y_pred)\n",
    "    # huber2 = tf.keras.losses.Huber(delta=0.1)(y_true_filter, y_pred_filter)\n",
    "    \n",
    "    # Cosine Similarity Loss\n",
    "    # 使用 tf.keras.losses.cosine_similarity，并确保结果为正值\n",
    "    cosine_loss = 1+tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    #cosine_loss2=tf.exp(cosine_loss)\n",
    "    \n",
    "    # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "    combined = 1000 * huber + 1*cosine_loss + 1000*wmse\n",
    "\n",
    "    return combined\n",
    "\n",
    "def Wight_huber_loss(y_true, y_pred):\n",
    "    \n",
    "    beta=1.0\n",
    "    weights = tf.where(tf.not_equal(y_true, 0), 1.0, 0.01)\n",
    "    diff = K.abs(y_true - y_pred)\n",
    "    loss = tf.where(K.less(diff, beta), weights*0.5 * K.square(diff) / beta, weights*diff - 0.5 * beta)\n",
    "    return K.mean(loss, axis=-1)\n",
    "    \n",
    "def sparsity(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred)) / K.max(K.square(y_pred))\n",
    "def combined_loss(y_true, y_pred):\n",
    "\n",
    "    print(y_pred.shape)\n",
    "    print(y_true.shape)\n",
    "    # MSE Loss\n",
    "    huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "\n",
    "    sparsity_loss = sparsity(y_true, y_pred)\n",
    "\n",
    "    print(sparsity_loss)\n",
    "    \n",
    "    # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "    combined = huber + 0.1 * sparsity_loss\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca07262f-68d0-4889-9a41-d4102e59e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define several different dropout functions\n",
    "class ChannelRegionDropout(Layer):\n",
    "    def __init__(self, block_size=3, **kwargs):\n",
    "        super(ChannelRegionDropout, self).__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "\n",
    "        def dropped_inputs():\n",
    "            input_shape = tf.shape(inputs)\n",
    "            batch_size = input_shape[0]\n",
    "            timesteps = input_shape[1]\n",
    "            channels = input_shape[2]\n",
    "\n",
    "            # 计算需要生成的掩码大小\n",
    "            mask_size = channels - self.block_size + 1\n",
    "\n",
    "            # 随机选择一个起始点\n",
    "            start_channel = tf.random.uniform(shape=(), minval=0, maxval=mask_size, dtype=tf.int32)\n",
    "\n",
    "            # 生成基础掩码\n",
    "            mask = tf.concat([\n",
    "                tf.ones((batch_size, timesteps, start_channel), dtype=inputs.dtype),\n",
    "                tf.zeros((batch_size, timesteps, self.block_size), dtype=inputs.dtype),\n",
    "                tf.ones((batch_size, timesteps, channels - start_channel - self.block_size), dtype=inputs.dtype)\n",
    "            ], axis=2)\n",
    "\n",
    "            return inputs * mask\n",
    "\n",
    "        return K.in_train_phase(dropped_inputs(), inputs, training=training)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class ChannelDropout(Layer):\n",
    "    def __init__(self, drop_rate, block_size, **kwargs):\n",
    "        super(ChannelDropout, self).__init__(**kwargs)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ChannelDropout, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            input_shape = tf.shape(inputs)\n",
    "            batch_size = input_shape[0]\n",
    "            timesteps = input_shape[1]\n",
    "            channels = input_shape[2]\n",
    "\n",
    "            # 计算缩减后的通道数\n",
    "            reduced_channels = (channels + self.block_size - 1) // self.block_size\n",
    "\n",
    "            # 生成基本掩码\n",
    "            random_tensor = tf.random.uniform((batch_size, timesteps, reduced_channels), dtype=inputs.dtype)\n",
    "            dropout_mask = tf.cast(random_tensor < (1 - self.drop_rate), inputs.dtype)\n",
    "\n",
    "            # 扩展掩码以覆盖原始维度\n",
    "            dropout_mask = tf.tile(dropout_mask, [1, 1, self.block_size])\n",
    "            # 裁剪掩码以匹配输入尺寸\n",
    "            dropout_mask = tf.slice(dropout_mask, [0, 0, 0], [batch_size, timesteps, channels])\n",
    "\n",
    "            # 应用掩码\n",
    "            return inputs * dropout_mask\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class CustomDropout(Layer):\n",
    "    def __init__(self, drop_rate, block_size_time, block_size_channel, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.block_size_time = block_size_time\n",
    "        self.block_size_channel = block_size_channel\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(CustomDropout, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            input_shape = tf.shape(inputs)\n",
    "            batch_size = input_shape[0]\n",
    "            timesteps = input_shape[1]\n",
    "            channels = input_shape[2]\n",
    "\n",
    "            # 计算缩减后的时间步和通道数\n",
    "            reduced_timesteps = (timesteps + self.block_size_time - 1) // self.block_size_time\n",
    "            reduced_channels = (channels + self.block_size_channel - 1) // self.block_size_channel\n",
    "\n",
    "            # 生成基本掩码\n",
    "            random_tensor = tf.random.uniform((batch_size, reduced_timesteps, reduced_channels), dtype=inputs.dtype)\n",
    "            dropout_mask = tf.cast(random_tensor < (1 - self.drop_rate), inputs.dtype)\n",
    "\n",
    "            # 扩展掩码以覆盖原始维度\n",
    "            dropout_mask = tf.tile(dropout_mask, [1, self.block_size_time, self.block_size_channel])\n",
    "            # 裁剪掩码以匹配输入尺寸\n",
    "            dropout_mask = tf.slice(dropout_mask, [0, 0, 0], [batch_size, timesteps, channels])\n",
    "\n",
    "            # 应用掩码\n",
    "            return inputs * dropout_mask\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class LateSpatialDropout1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, start_step=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.rate = rate\n",
    "        self.start_step = start_step\n",
    "        self.spatial_dropout = tf.keras.layers.SpatialDropout1D(rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.cond(self._train_counter < self.start_step,\n",
    "                    lambda: inputs,\n",
    "                    lambda: self.spatial_dropout(inputs, training=training))\n",
    "\n",
    "        if training:\n",
    "            self._train_counter.assign_add(1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde9ac92-78d9-45d0-a7f7-e69d14451a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对输入eeg进行归一化，采用Z分数标准化通常是首选，因为它使数据具有统一的尺度，而且能较好地处理数据的异常值和噪声\n",
    "def custom_prep_data(data):\n",
    "\n",
    "    data = np.swapaxes(data, 1,2)\n",
    "\n",
    "    # 获取数据维度\n",
    "    num_samples, num_timepoints, num_channels = data.shape\n",
    "    \n",
    "    # 将数据类型转换为 np.float32\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    # 对每个样本（脑电信号的一个时间点）进行处理\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_timepoints):\n",
    "            # 获取当前样本的数据\n",
    "            sample_data = data[i, j, :]\n",
    "            \n",
    "            # 假设你想要进行去除平均值和标准化的预处理\n",
    "            # 去除平均值\n",
    "            sample_data_mean = np.mean(sample_data)\n",
    "            sample_data_std = np.std(sample_data)\n",
    "            sample_data -= sample_data_mean\n",
    "            \n",
    "            # 标准化\n",
    "            if sample_data_std != 0:\n",
    "                sample_data /= sample_data_std\n",
    "        \n",
    "            # 更新数据\n",
    "            data[i, j, :] = sample_data\n",
    "    print(\"The shape of EEG is\", data.shape)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 对源信号进行归一化，这种最大绝对值归一化能够保持数据的结构和正负性，仅改变数据的尺度。这意味着信号的正负模式被保留。\n",
    "def custom_prep_source(data):\n",
    "    \n",
    "    # 将数据类型转换为 np.float32\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    for i, y_sample in enumerate(data):\n",
    "        max_abs_vals=np.array(np.max(abs(data[i])))\n",
    "        max_abs_vals[max_abs_vals == 0] = 1\n",
    "        data[i] /= max_abs_vals   \n",
    "\n",
    "    data = np.swapaxes(data, 1,2)\n",
    "    print(\"The shape of source is\", data.shape)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f287b6a0-fe37-433e-986c-815976091200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义学习率调度函数\n",
    "def lr_schedule(epoch):\n",
    "    # 根据训练周期(epoch)来动态调整学习率\n",
    "    if epoch < 50:\n",
    "        return 0.0003  \n",
    "    elif epoch < 100:\n",
    "        return 0.0002 \n",
    "    elif epoch < 150:\n",
    "        return 0.0001  \n",
    "    else:\n",
    "        return 0.00005 \n",
    "\n",
    "# 创建学习率调度器,损失函数有余弦相似度 CosineSimilarity, tf.keras.losses.Huber(), MeanAbsoluteError, MeanSquaredError\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dea1584-d54a-43a4-93e1-343f9644802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simplified_Hybrid_Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, None, 60)]   0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 200)    12200       ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 200)    40200       ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, None, 200)    40200       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, None, 200)    0           ['dense[0][0]',                  \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.tanh (TFOpLambda)      (None, None, 200)    0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 200)    0           ['tf.math.tanh[0][0]']           \n",
      "                                                                                                  \n",
      " BiLSTM1 (Bidirectional)        (None, None, 200)    128800      ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, None, 400)    0           ['dropout[0][0]',                \n",
      "                                                                  'BiLSTM1[0][0]']                \n",
      "                                                                                                  \n",
      " BiLSTM2 (Bidirectional)        (None, None, 400)    961600      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, None, 400)   800         ['BiLSTM2[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, None, 400)   821136      ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, None, 400)    0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'BiLSTM2[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, None, 400)   800         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 600)   240600      ['layer_normalization_1[0][0]']  \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, None, 600)    0           ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, None, 400)   240400      ['dropout_1[0][0]']              \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, None, 400)    0           ['time_distributed_1[0][0]',     \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " Direct_Output (TimeDistributed  (None, None, 844)   338444      ['add_2[0][0]']                  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 844)         0           ['Direct_Output[0][0]']          \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 105)          88725       ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 844)          89464       ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 844)       0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " multiply2 (Multiply)           (None, None, 844)    0           ['Direct_Output[0][0]',          \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,003,369\n",
      "Trainable params: 3,003,369\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 超参数定义, kernel_initializer='he_uniform', kernel_initializer='glorot_uniform' dropout=dropout_rate\n",
    "n_channels = 60\n",
    "n_dipoles = 844\n",
    "n_dense_units = 200  # 减少单元数 200\n",
    "n_lstm_units1 = 100   # 减少单元数 100\n",
    "n_lstm_units2 = 200    # 减少单元数 100\n",
    "dropout_rate = 0.2 # 0.2\n",
    "batch_size = 32 # 32 2048 2的次幂\n",
    "epochs = 180\n",
    "noise_factor=0.1\n",
    "# 导入邻接矩阵V\n",
    "V=np.load('V.npy')\n",
    "# 导入拉普拉斯矩阵L\n",
    "L=np.load('L.npy')\n",
    "\n",
    "# 输入层\n",
    "inputs = Input(shape=(None, n_channels), name='Input')\n",
    "#noisy_input = GaussianNoise(stddev=noise_factor)(inputs, training=True)\n",
    "# 并行处理输入\n",
    "fc1 = Dense(n_dense_units, activation=\"relu\")(inputs)\n",
    "fc2 = Dense(n_dense_units, activation=\"relu\")(fc1)\n",
    "fc3 = Dense(n_dense_units, activation=\"relu\")(fc2)\n",
    "add1 = Add()([fc1, fc3])\n",
    "add1 = tanh(add1)\n",
    "add1 = Dropout(dropout_rate)(add1)\n",
    "bi_lstm = Bidirectional(LSTM(n_lstm_units1, return_sequences=True, dropout=dropout_rate), name='BiLSTM1')(inputs)\n",
    "\n",
    "# 合并并行层的输出\n",
    "merged = concatenate([add1, bi_lstm], axis=-1)\n",
    "lstm = Bidirectional(LSTM(n_lstm_units2, return_sequences=True), name='BiLSTM2')(merged)\n",
    "\n",
    "# Transformer层\n",
    "transformer_block = transformer_encoder(lstm, head_size=64, num_heads=8, ff_dim=600)\n",
    "\n",
    "# 输出层\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, activation=\"linear\"), name='Direct_Output')(transformer_block)\n",
    "CATT = channel_attention_module(direct_out)\n",
    "\n",
    "# skip_connection = Dense(n_dipoles, activation=\"tanh\")(inputs)  # 需要通过一个调整层来适配维度\n",
    "# outputs = Add()([CATT, skip_connection])\n",
    "\n",
    "# 创建和编译模型\n",
    "model = tf.keras.Model(inputs=inputs, outputs=CATT, name='Simplified_Hybrid_Model')\n",
    "model.compile(loss=wight_combined_loss2, optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003))\n",
    "\n",
    "# 打印模型概要\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f45b428-7827-45be-b1c8-2658493a6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载测试数据\n",
    "x= np.load('x_daodianlv.npy')\n",
    "y= np.load('y_daodianlv.npy')\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb47b7-8abf-4c92-9934-87a5125b3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 对信号进行预处理\n",
    "x = custom_prep_data(x)\n",
    "y = custom_prep_source(y)\n",
    "# y = np.swapaxes(y, 1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10311a34-042b-4c73-91d7-c7b88295f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用数据生成器进行训练\n",
    "def data_generator(x, y, batch_size):\n",
    "    num_samples = len(x)\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            yield x[batch_indices], y[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9dc45c-bb55-4420-a472-15c125689bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "split_index = int(0.9 * len(x))\n",
    "x_train, x_val = x[:split_index], x[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c1691-4363-4f67-94e6-b98e1abd9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators for training and validation\n",
    "train_generator = data_generator(x_train, y_train, batch_size)\n",
    "val_generator = data_generator(x_val, y_val, batch_size)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a36e7c-5b6b-4b09-ba24-d01f529a64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个epoch的步骤和验证步骤\n",
    "steps_per_epoch = len(x_train) // batch_size\n",
    "validation_steps = len(x_val) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa333345-1fbc-4855-b857-b93b95f5649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型时使用学习率调度器\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, lr_scheduler]  # 添加学习率调度器回调\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d7c34-9013-4fe1-902b-cb13924bca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制验证损失和训练损失\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置字体大小\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# 设置图像大小\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 绘制训练和验证损失\n",
    "plt.plot(history.history['loss'], label='训练损失')\n",
    "plt.plot(history.history['val_loss'], label='验证损失')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('损失')\n",
    "plt.title('原始模型 训练和验证损失')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d1676-dd80-43c3-9ae9-dea85884889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save('model_daodianlv', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c62ba1-e5e2-42fa-8f8c-615979bc48f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
