{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3367e635-b5d7-42d1-8c0d-170f23c6c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, multiply, Input, Dropout, Conv1D, Flatten, GRU, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Add, concatenate, GaussianNoise, Layer, BatchNormalization, Reshape, Activation\n",
    "from tensorflow.keras.regularizers import l2, l1\n",
    "from tensorflow.keras.activations import relu, elu, tanh\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from esinet import util\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from esinet import Simulation, Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "33a25b71-8569-4482-9e34-3781680726cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention_module(input_tensor, reduction_ratio=8):\n",
    "    # 获取输入通道数\n",
    "    channels = input_tensor.shape[-1]\n",
    "    \n",
    "    # 压缩阶段\n",
    "    squeeze = GlobalAveragePooling1D()(input_tensor)\n",
    "    \n",
    "    # 激励阶段\n",
    "    excitation = Dense(channels // reduction_ratio, activation='relu')(squeeze)\n",
    "    excitation = Dense(channels, activation='sigmoid')(excitation)\n",
    "    \n",
    "    # 将激励层的输出调整形状以匹配原始输入的形状\n",
    "    excitation = Reshape((1, channels))(excitation)\n",
    "    \n",
    "    # 重标定阶段，逐元素乘法\n",
    "    scale = multiply([input_tensor, excitation],  name=\"multiply2\")\n",
    "    \n",
    "    \n",
    "    return scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c76e4992-ebe0-4619-807f-aed6ec191c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.2):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Add()([x, inputs])\n",
    "\n",
    "    # Feed Forward Part\n",
    "    ff = LayerNormalization(epsilon=1e-6)(x)\n",
    "    ff = Dense(ff_dim, activation=\"relu\")(ff)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    ff = Add()([ff, x])\n",
    "\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b23856cb-f2fc-4148-9c1c-e8dc38ababf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define several different loss functions\n",
    "\n",
    "def combined_loss_Laplactransform(L):\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # MSE Loss\n",
    "        huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "        \n",
    "        # Cosine Similarity Loss\n",
    "        # 使用 tf.keras.losses.cosine_similarity，并确保结果为正值\n",
    "        cosine_loss = 1+tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    \n",
    "        # 计算拉普拉斯正则化项\n",
    "        # y_pred需要与拉普拉斯矩阵的维度匹配\n",
    "        laplacian_reg = tf.matmul(tf.matmul(y_pred, L), y_pred, transpose_a=True)\n",
    "        l1_loss = tf.reduce_mean(tf.abs(laplacian_reg))\n",
    "        \n",
    "        # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "        combined = 10000 * huber + 3*cosine_loss + 100*l1_loss\n",
    "        \n",
    "        return combined\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def l2_loss(weights, lambda_l2=0.1):\n",
    "    # 对所有权重应用L2损失\n",
    "    return lambda_l2 * K.sum(K.square(weights))\n",
    "\n",
    "def wight_huber_loss(lambda_l2=0.1):\n",
    "    def loss(y_true, y_pred):\n",
    "        # 计算 MSE 损失\n",
    "        huber = tf.keras.losses.Huber(delta=0.5)(y_true, y_pred)\n",
    "        cosine_loss = 1+tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "        \n",
    "        # 计算L2损失，针对模型的权重\n",
    "        l2_regularization = 0\n",
    "        for layer in y_pred.model.layers:  # 遍历所有层\n",
    "            if hasattr(layer, 'kernel'):  # 权重（kernel）是层的一部分\n",
    "                l2_regularization += l2_loss(layer.kernel, lambda_l2)\n",
    "\n",
    "        # 合并MSE和L2损失\n",
    "        total_loss = 1000 * huber + cosine_loss + l2_regularization\n",
    "        return l2_regularization\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def wight_combined_loss2(y_true, y_pred):\n",
    "    \n",
    "    # 设定权重给真实值为0的位置的误差赋予加权因子w\n",
    "    weights = tf.where(tf.not_equal(y_true, 0), 1.0, 0.01)\n",
    "    # 计算加权MSE\n",
    "    wmse=tf.reduce_mean(weights * tf.square(y_true - y_pred))\n",
    "    # 为了避免标签的大小影响误差的计算，进行归一化处理。我们对每个样本的误差除以其真实标签的 L2范数 \n",
    "    # 在某些任务中，真实标签中有很多 0，这可能意味着这些位置的数据不重要或被忽略。加权损失可以对这些位置加权或平衡损失。\n",
    "\n",
    "    # MSE Loss\n",
    "    huber = tf.keras.losses.Huber(delta=0.5)(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    # Cosine Similarity Loss\n",
    "    # 使用 tf.keras.losses.cosine_similarity，并确保结果为正值\n",
    "    cosine_loss = 1+ tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    \n",
    "    # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "    combined = 1000 * huber + cosine_loss + 1000*wmse\n",
    "\n",
    "    return combined\n",
    "\n",
    "def Wight_huber_loss(y_true, y_pred):\n",
    "    \n",
    "    beta=1.0\n",
    "    weights = tf.where(tf.not_equal(y_true, 0), 1.0, 0.01)\n",
    "    diff = K.abs(y_true - y_pred)\n",
    "    loss = tf.where(K.less(diff, beta), weights*0.5 * K.square(diff) / beta, weights*diff - 0.5 * beta)\n",
    "    return K.mean(loss, axis=-1)\n",
    "    \n",
    "def sparsity(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred)) / K.max(K.square(y_pred))\n",
    "def combined_loss(y_true, y_pred):\n",
    "\n",
    "    print(y_pred.shape)\n",
    "    print(y_true.shape)\n",
    "    # MSE Loss\n",
    "    huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "\n",
    "    sparsity_loss = sparsity(y_true, y_pred)\n",
    "\n",
    "    print(sparsity_loss)\n",
    "    \n",
    "    # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "    combined = huber + 0.1 * sparsity_loss\n",
    "    return combined\n",
    "\n",
    "def sparse_kl_loss(y_true, y_pred, target_sparsity):\n",
    "    \"\"\"\n",
    "    稀疏性惩罚项，使用 KL 散度来衡量目标稀疏度和实际稀疏度之间的差异。\n",
    "    \n",
    "    参数：\n",
    "    y_true: 真实标签（目标向量）\n",
    "    y_pred: 模型预测输出\n",
    "    target_sparsity: 目标稀疏度分布（例如，可以通过 L1 范数等方法创建）\n",
    "    \n",
    "    返回：\n",
    "    总损失（包括稀疏性惩罚项）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 假设 y_true 和 y_pred 都是 shape (batch_size, time_steps, channels) 的张量\n",
    "    # 计算预测的 L1 范数（稀疏性度量）\n",
    "    pred_sparsity = K.sum(K.abs(y_pred), axis=-1)\n",
    "    \n",
    "    # 归一化稀疏度为概率分布（可以使用 softmax 来确保它们是有效的概率分布）\n",
    "    pred_sparsity = K.softmax(pred_sparsity, axis=-1)\n",
    "    target_sparsity = K.softmax(target_sparsity, axis=-1)\n",
    "    \n",
    "    # 计算 KL 散度损失\n",
    "    kl_loss = K.sum(target_sparsity * K.log((target_sparsity + 1e-10) / (pred_sparsity + 1e-10)), axis=-1)\n",
    "    \n",
    "    return kl_loss\n",
    "\n",
    "# 同时考虑EEG和源信号损失\n",
    "leadfield= np.load('leadfield.npy') # 提取导联矩阵，从而用来生成EEG\n",
    "def data_loss(leadfield, lam_0=1):\n",
    "    leadfield_ = tf.cast(leadfield, dtype=tf.float32)\n",
    "    def batch_data_loss(y_true, y_est):\n",
    "        def d_loss(y_true, y_est):\n",
    "            y_true_eeg = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_true)))\n",
    "            y_est_eeg = tf.transpose(tf.matmul(leadfield_, tf.transpose(y_est)))\n",
    "            # print(\"y_true \", y_true)\n",
    "            # print(\"y_est \", y_est)\n",
    "            error_source1 = tf.keras.losses.Huber(name=\"Source_Data_Cosine_Loss\")(y_est, y_true)\n",
    "            error_source2 = 1+ tf.keras.losses.CosineSimilarity(name=\"Source_Data_Cosine_Loss\")(y_est, y_true)\n",
    "            error_source = 1000 * error_source1 + error_source2\n",
    "            error_eeg1 = 1+ tf.keras.losses.CosineSimilarity(name=\"EEG_Data_Cosine_Loss\")(y_est_eeg, y_true_eeg)\n",
    "            error_eeg2 = tf.keras.losses.Huber(name=\"Source_Data_Cosine_Loss\")(y_est_eeg, y_true_eeg)\n",
    "            error_eeg = error_eeg1 + 0.0001 * error_eeg2\n",
    "            return error_source + error_eeg1 * lam_0\n",
    "\n",
    "        batched_losses = tf.map_fn(lambda x:\n",
    "            d_loss(x[0], x[1]), \n",
    "            (y_true, y_est), dtype=tf.float32)\n",
    "        return K.mean(batched_losses)\n",
    "\n",
    "    return batch_data_loss\n",
    "\n",
    "# 引入多尺度损失，使用小波变换将信号分解为多个频带，然后在每个频带上计算损失。可以有效捕捉信号中的不同频带信息，提升对细节的还原。\n",
    "def wavelet_transform_loss(y_true, y_est):\n",
    "    # 对信号进行小波变换\n",
    "    coeffs_true = pywt.wavedec(y_true, 'db1', level=4)\n",
    "    coeffs_est = pywt.wavedec(y_est, 'db1', level=4)\n",
    "    \n",
    "    # 计算每个尺度的MSE\n",
    "    loss = 0\n",
    "    for c_true, c_est in zip(coeffs_true, coeffs_est):\n",
    "        loss += tf.reduce_mean(tf.square(c_true - c_est))\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gating_layer(x,\n",
    "                       hidden_layer_size,\n",
    "                       dropout_rate=None,\n",
    "                       use_time_distributed=True,\n",
    "                       activation=None):\n",
    "\n",
    "  if dropout_rate is not None:\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "  if use_time_distributed:\n",
    "    activation_layer = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation=activation))(\n",
    "            x)\n",
    "    gated_layer = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'))(\n",
    "            x)\n",
    "\n",
    "  return tf.keras.layers.Multiply()([activation_layer,\n",
    "                                     gated_layer]), gated_layer\n",
    "\n",
    "\n",
    "def add_and_norm(x_list):\n",
    "    \n",
    "  tmp = Add()(x_list)\n",
    "  tmp = LayerNorm()(tmp)\n",
    "  return tmp\n",
    "\n",
    "\n",
    "def gated_residual_network(x,\n",
    "                           hidden_layer_size,\n",
    "                           output_size=None,\n",
    "                           dropout_rate=None,\n",
    "                           use_time_distributed=True,\n",
    "                           additional_context=None,\n",
    "                           return_gate=False):\n",
    "\n",
    "  # Setup skip connection\n",
    "  if output_size is None:\n",
    "    output_size = hidden_layer_size\n",
    "    skip = x\n",
    "  else:\n",
    "    linear = Dense(output_size)\n",
    "    if use_time_distributed:\n",
    "      linear = tf.keras.layers.TimeDistributed(linear)\n",
    "    skip = linear(x)\n",
    "\n",
    "  # Apply feedforward network\n",
    "  hidden = linear_layer(\n",
    "      hidden_layer_size,\n",
    "      activation=None,\n",
    "      use_time_distributed=use_time_distributed)(\n",
    "          x)\n",
    "  hidden = tf.keras.layers.Activation('elu')(hidden)\n",
    "  hidden = linear_layer(\n",
    "      hidden_layer_size,\n",
    "      activation=None,\n",
    "      use_time_distributed=use_time_distributed)(\n",
    "          hidden)\n",
    "\n",
    "  gating_layer, gate = apply_gating_layer(\n",
    "      hidden,\n",
    "      output_size,\n",
    "      dropout_rate=dropout_rate,\n",
    "      use_time_distributed=use_time_distributed,\n",
    "      activation=None)\n",
    "\n",
    "  if return_gate:\n",
    "    return add_and_norm([skip, gating_layer]), gate\n",
    "  else:\n",
    "    return add_and_norm([skip, gating_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fde9ac92-78d9-45d0-a7f7-e69d14451a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对输入eeg进行归一化，采用Z分数标准化通常是首选，因为它使数据具有统一的尺度，而且能较好地处理数据的异常值和噪声\n",
    "def custom_prep_data(data):\n",
    "\n",
    "    data = np.swapaxes(data, 1,2)\n",
    "\n",
    "    # 获取数据维度\n",
    "    num_samples, num_timepoints, num_channels = data.shape\n",
    "    \n",
    "    # 将数据类型转换为 np.float32\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    # 对每个样本（脑电信号的一个时间点）进行处理\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_timepoints):\n",
    "            # 获取当前样本的数据\n",
    "            sample_data = data[i, j, :]\n",
    "            \n",
    "            # 假设你想要进行去除平均值和标准化的预处理\n",
    "            # 去除平均值\n",
    "            sample_data_mean = np.mean(sample_data)\n",
    "            sample_data_std = np.std(sample_data)\n",
    "            sample_data -= sample_data_mean\n",
    "            \n",
    "            # 标准化\n",
    "            if sample_data_std != 0:\n",
    "                sample_data /= sample_data_std\n",
    "        \n",
    "            # 更新数据\n",
    "            data[i, j, :] = sample_data\n",
    "    print(\"The shape of EEG is\", data.shape)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 对源信号进行归一化，这种最大绝对值归一化能够保持数据的结构和正负性，仅改变数据的尺度。这意味着信号的正负模式被保留。\n",
    "def custom_prep_source(data):\n",
    "    \n",
    "    # 将数据类型转换为 np.float32\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    for i, y_sample in enumerate(data):\n",
    "        max_abs_vals=np.array(np.max(abs(data[i])))\n",
    "        max_abs_vals[max_abs_vals == 0] = 1\n",
    "        data[i] /= max_abs_vals   \n",
    "\n",
    "    data = np.swapaxes(data, 1,2)\n",
    "    print(\"The shape of source is\", data.shape)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f287b6a0-fe37-433e-986c-815976091200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义学习率调度函数\n",
    "def lr_schedule(epoch):\n",
    "    # 根据训练周期(epoch)来动态调整学习率\n",
    "    if epoch < 50:\n",
    "        return 0.0003  \n",
    "    elif epoch < 100:\n",
    "        return 0.0002 \n",
    "    elif epoch < 125:\n",
    "        return 0.0001\n",
    "    elif epoch < 150:\n",
    "        return 0.00005\n",
    "    else:\n",
    "        return 0.00001 \n",
    "    \n",
    "def lr_schedule2(epoch):\n",
    "    # 根据训练周期(epoch)来动态调整学习率\n",
    "    if epoch < 50:\n",
    "        return 0.0001  \n",
    "    elif epoch < 100:\n",
    "        return 0.00001 \n",
    "    elif epoch < 125:\n",
    "        return 0.000005\n",
    "    else:\n",
    "        return 0.000001\n",
    "\n",
    "# 创建学习率调度器,损失函数有余弦相似度 CosineSimilarity, tf.keras.losses.Huber(), MeanAbsoluteError, MeanSquaredError\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "deff5718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 epoch 逐渐增加，训练过程中的权重逐步从源信号转向 EEG 信号\n",
    "class DynamicLossWeight(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, cos_initial_weight=1.0, cos_final_weight=0.0, huber_initial_weight=0.0, huber_final_weight=1.0, total_epochs = 180):\n",
    "        super(DynamicLossWeight, self).__init__()\n",
    "        self.cos_initial_weight = cos_initial_weight\n",
    "        self.cos_final_weight = cos_final_weight\n",
    "        self.huber_initial_weight = huber_initial_weight\n",
    "        self.huber_final_weight = huber_final_weight\n",
    "        self.total_epochs = total_epochs\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 计算当前 epoch 对应的损失权重\n",
    "        cos_weight = self.cos_initial_weight - (self.cos_initial_weight - self.cos_final_weight) * (epoch / self.total_epochs)\n",
    "        huber_weight = self.huber_initial_weight + (self.huber_final_weight - self.huber_initial_weight) * (epoch / self.total_epochs)\n",
    "        \n",
    "        # 更新模型中的 MSE 损失的权重\n",
    "        if 'cos_loss_weight' in self.model.losses:\n",
    "            self.model.losses['cos_loss_weight'] = cos_weight\n",
    "        else:\n",
    "            self.model.losses.append(('cos_loss_weight', cos_weight))\n",
    "        \n",
    "        # 更新模型中的 Huber 损失的权重\n",
    "        if 'huber_loss_weight' in self.model.losses:\n",
    "            self.model.losses['huber_loss_weight'] = huber_weight\n",
    "        else:\n",
    "            self.model.losses.append(('huber_loss_weight', huber_weight))\n",
    "\n",
    "        # 记录当前的权重\n",
    "        logs['cos_loss_weight'] = cos_weight\n",
    "        logs['huber_loss_weight'] = huber_weight\n",
    "\n",
    "        print(f'\\nEpoch {epoch + 1}: cos loss weight is {cos_weight:.4f}, Huber loss weight is {huber_weight:.4f}')\n",
    "        \n",
    "def weighted_loss(y_true, y_pred, cos_weight=1.0, huber_weight=0.0):\n",
    "    huber_loss = huber_weight * tf.keras.losses.Huber(delta=1)(y_true, y_pred)\n",
    "    cos_loss = cos_weight * (1 + tf.keras.losses.CosineSimilarity(y_true, y_pred))\n",
    "\n",
    "    return cos_loss + huber_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7dea1584-d54a-43a4-93e1-343f9644802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simplified_Hybrid_Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, None, 60)]   0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_114 (TimeDist  (None, None, 60)    3660        ['Input[0][0]']                  \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_115 (TimeDist  (None, None, 128)   7808        ['time_distributed_114[0][0]']   \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, None, 128)    0           ['time_distributed_115[0][0]']   \n",
      "                                                                                                  \n",
      " time_distributed_117 (TimeDist  (None, None, 128)   7808        ['time_distributed_114[0][0]']   \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_116 (TimeDist  (None, None, 128)   16512       ['dropout_36[0][0]']             \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " add_45 (Add)                   (None, None, 128)    0           ['time_distributed_117[0][0]',   \n",
      "                                                                  'time_distributed_116[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_32 (LayerN  (None, None, 128)   256         ['add_45[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " time_distributed_118 (TimeDist  (None, None, 128)   16512       ['layer_normalization_32[0][0]'] \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_119 (TimeDist  (None, None, 256)   33024       ['time_distributed_118[0][0]']   \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, None, 256)    0           ['time_distributed_119[0][0]']   \n",
      "                                                                                                  \n",
      " time_distributed_121 (TimeDist  (None, None, 256)   33024       ['time_distributed_118[0][0]']   \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_120 (TimeDist  (None, None, 256)   65792       ['dropout_37[0][0]']             \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " add_46 (Add)                   (None, None, 256)    0           ['time_distributed_121[0][0]',   \n",
      "                                                                  'time_distributed_120[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_33 (LayerN  (None, None, 256)   512         ['add_46[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " time_distributed_122 (TimeDist  (None, None, 256)   65792       ['layer_normalization_33[0][0]'] \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_123 (TimeDist  (None, None, 512)   131584      ['time_distributed_122[0][0]']   \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, None, 512)    0           ['time_distributed_123[0][0]']   \n",
      "                                                                                                  \n",
      " time_distributed_125 (TimeDist  (None, None, 512)   131584      ['time_distributed_122[0][0]']   \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_124 (TimeDist  (None, None, 512)   262656      ['dropout_38[0][0]']             \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " add_47 (Add)                   (None, None, 512)    0           ['time_distributed_125[0][0]',   \n",
      "                                                                  'time_distributed_124[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_34 (LayerN  (None, None, 512)   1024        ['add_47[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_9 (MultiH  (None, None, 512)   1050624     ['layer_normalization_34[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " add_48 (Add)                   (None, None, 512)    0           ['multi_head_attention_9[0][0]', \n",
      "                                                                  'add_47[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_35 (LayerN  (None, None, 512)   1024        ['add_48[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_147 (Dense)              (None, None, 512)    262656      ['layer_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, None, 512)    0           ['dense_147[0][0]']              \n",
      "                                                                                                  \n",
      " dense_148 (Dense)              (None, None, 512)    262656      ['dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      " add_49 (Add)                   (None, None, 512)    0           ['dense_148[0][0]',              \n",
      "                                                                  'add_48[0][0]']                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BiLSTM1 (Bidirectional)        (None, None, 1024)   4198400     ['add_49[0][0]']                 \n",
      "                                                                                                  \n",
      " BiLSTM2 (Bidirectional)        (None, None, 1024)   6295552     ['BiLSTM1[0][0]']                \n",
      "                                                                                                  \n",
      " Direct_Output (TimeDistributed  (None, None, 1284)  1316100     ['BiLSTM2[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,164,560\n",
      "Trainable params: 14,164,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 超参数定义, kernel_initializer='he_uniform', kernel_initializer='glorot_uniform' dropout=dropout_rate\n",
    "n_channels = 60\n",
    "n_dipoles = 1284\n",
    "n_dense1 = 128\n",
    "n_dense2 = 256\n",
    "n_dense3 = 512\n",
    "dropout_rate = 0.2 # 0.2\n",
    "batch_size = 32 # 32 2048 2的次幂\n",
    "epochs = 180\n",
    "noise_factor=0.2\n",
    "# 导入邻接矩阵V\n",
    "V=np.load('V.npy')\n",
    "# 导入拉普拉斯矩阵L\n",
    "L=np.load('L.npy')\n",
    "lrelu = tf.keras.layers.LeakyReLU()\n",
    "\n",
    "# 输入层\n",
    "inputs = Input(shape=(None, n_channels), name='Input')\n",
    "# 并行处理输入\n",
    "fc1 = TimeDistributed(Dense(n_channels, activation=\"elu\"))(inputs)\n",
    "fc2_d = Dropout(dropout_rate)(fc1)\n",
    "fc2_w = TimeDistributed(Dense(n_dense1, activation=\"linear\"))(fc2_d)\n",
    "fc2_g = TimeDistributed(Dense(n_dense1, activation=\"sigmoid\"))(fc2_d)\n",
    "fc2 = multiply()[fc2_w, fc2_g]\n",
    "fc3 = TimeDistributed(Dense(n_dense1, activation=\"elu\"))(fc2)\n",
    "fc1_w = TimeDistributed(Dense(n_dense1, activation=\"linear\"))(fc1)\n",
    "add1 = Add()([fc1_w, fc3])\n",
    "add1 = LayerNormalization(epsilon=1e-6)(add1)\n",
    "\n",
    "fc4 = TimeDistributed(Dense(n_dense1, activation=\"elu\"))(add1)\n",
    "fc5_d = Dropout(dropout_rate)(fc4)\n",
    "fc5_w = TimeDistributed(Dense(n_dense2, activation=\"linear\"))(fc5_d)\n",
    "fc5_g = TimeDistributed(Dense(n_dense2, activation=\"sigmoid\"))(fc5_d)\n",
    "fc5 = multiply()[fc5_w, fc5_g]\n",
    "fc6 = TimeDistributed(Dense(n_dense2, activation=\"elu\"))(fc5)\n",
    "fc4_w = TimeDistributed(Dense(n_dense2, activation=\"linear\"))(fc4)\n",
    "add2 = Add()([fc4_w, fc6])\n",
    "add2 = LayerNormalization(epsilon=1e-6)(add2)\n",
    "\n",
    "fc7 = TimeDistributed(Dense(n_dense2, activation=\"elu\"))(add2)\n",
    "fc8_d = Dropout(dropout_rate)(fc7)\n",
    "fc8_w = TimeDistributed(Dense(n_dense1, activation=\"linear\"))(fc8_d)\n",
    "fc8_g = TimeDistributed(Dense(n_dense1, activation=\"sigmoid\"))(fc8_d)\n",
    "fc8 = multiply()[fc8_w, fc8_g]\n",
    "fc9 = TimeDistributed(Dense(n_dense3, activation=\"elu\"))(fc8)\n",
    "fc7_w = TimeDistributed(Dense(n_dense3, activation=\"linear\"))(fc7)\n",
    "add3 = Add()([fc7_w, fc9])\n",
    "\n",
    "transformer_block = transformer_encoder(add3, head_size=128, num_heads=4, ff_dim=n_dense3)\n",
    "\n",
    "lstm1 = Bidirectional(LSTM(n_dense3, return_sequences=True, dropout=dropout_rate), name='BiLSTM1')(transformer_block)\n",
    "lstm2 = Bidirectional(LSTM(n_dense3, return_sequences=True, dropout=dropout_rate), name='BiLSTM2')(lstm1)\n",
    "\n",
    "# transformer_block = transformer_encoder(lstm2, head_size=128, num_heads=4, ff_dim=n_dense3)\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, activation=\"linear\"), name='Direct_Output')(lstm2)\n",
    "\n",
    "# 创建和编译模型 loss=data_loss(leadfield, lam_0=0.1) loss=lambda y_true, y_pred: weighted_loss(y_true, y_pred, cos_weight=1.0, huber_weight=0.0)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=direct_out, name='Simplified_Hybrid_Model')\n",
    "model.compile(loss=data_loss(leadfield, lam_0=0.1), optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "# 打印模型概要\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0f45b428-7827-45be-b1c8-2658493a6128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 60, 26)\n",
      "(20000, 1284, 26)\n"
     ]
    }
   ],
   "source": [
    "# 加载测试数据\n",
    "x= np.load('x10.npy')\n",
    "y= np.load('y10.npy')\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c1cb47b7-8abf-4c92-9934-87a5125b3e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of EEG is (20000, 26, 60)\n",
      "The shape of source is (20000, 26, 1284)\n"
     ]
    }
   ],
   "source": [
    " # 对信号进行预处理\n",
    "x = custom_prep_data(x)\n",
    "y = custom_prep_source(y)\n",
    "# y = np.swapaxes(y, 1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "10311a34-042b-4c73-91d7-c7b88295f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用数据生成器进行训练\n",
    "def data_generator(x, y, batch_size):\n",
    "    num_samples = len(x)\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            yield x[batch_indices], y[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "3d9dc45c-bb55-4420-a472-15c125689bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "split_index = int(0.8 * len(x))\n",
    "x_train, x_val = x[:split_index], x[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "361c1691-4363-4f67-94e6-b98e1abd9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators for training and validation\n",
    "train_generator = data_generator(x_train, y_train, batch_size)\n",
    "val_generator = data_generator(x_val, y_val, batch_size)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=50, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "34a36e7c-5b6b-4b09-ba24-d01f529a64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个epoch的步骤和验证步骤\n",
    "steps_per_epoch = len(x_train) // batch_size\n",
    "validation_steps = len(x_val) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa333345-1fbc-4855-b857-b93b95f5649b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "500/500 [==============================] - 164s 299ms/step - loss: 2.5025 - val_loss: 2.2124 - lr: 3.0000e-04\n",
      "Epoch 2/180\n",
      "500/500 [==============================] - 142s 283ms/step - loss: 2.1052 - val_loss: 1.8998 - lr: 3.0000e-04\n",
      "Epoch 3/180\n",
      "500/500 [==============================] - 143s 286ms/step - loss: 1.8439 - val_loss: 1.6679 - lr: 3.0000e-04\n",
      "Epoch 4/180\n",
      "500/500 [==============================] - 141s 282ms/step - loss: 1.6591 - val_loss: 1.5250 - lr: 3.0000e-04\n",
      "Epoch 5/180\n",
      "500/500 [==============================] - 140s 279ms/step - loss: 1.5456 - val_loss: 1.4342 - lr: 3.0000e-04\n",
      "Epoch 6/180\n",
      "500/500 [==============================] - 143s 286ms/step - loss: 1.4654 - val_loss: 1.3804 - lr: 3.0000e-04\n",
      "Epoch 7/180\n",
      "500/500 [==============================] - 139s 279ms/step - loss: 1.4107 - val_loss: 1.3332 - lr: 3.0000e-04\n",
      "Epoch 8/180\n",
      "500/500 [==============================] - 143s 285ms/step - loss: 1.3641 - val_loss: 1.2982 - lr: 3.0000e-04\n",
      "Epoch 9/180\n",
      "500/500 [==============================] - 140s 281ms/step - loss: 1.3255 - val_loss: 1.2691 - lr: 3.0000e-04\n",
      "Epoch 10/180\n",
      "500/500 [==============================] - 141s 282ms/step - loss: 1.2918 - val_loss: 1.2596 - lr: 3.0000e-04\n",
      "Epoch 11/180\n",
      "500/500 [==============================] - 142s 284ms/step - loss: 1.2786 - val_loss: 1.2432 - lr: 3.0000e-04\n",
      "Epoch 12/180\n",
      "500/500 [==============================] - 141s 282ms/step - loss: 1.2490 - val_loss: 1.2380 - lr: 3.0000e-04\n",
      "Epoch 13/180\n",
      "500/500 [==============================] - 142s 284ms/step - loss: 1.2210 - val_loss: 1.2051 - lr: 3.0000e-04\n",
      "Epoch 14/180\n",
      "500/500 [==============================] - 142s 284ms/step - loss: 1.1962 - val_loss: 1.1970 - lr: 3.0000e-04\n",
      "Epoch 15/180\n",
      "500/500 [==============================] - 143s 286ms/step - loss: 1.1784 - val_loss: 1.1930 - lr: 3.0000e-04\n",
      "Epoch 16/180\n",
      "500/500 [==============================] - 144s 288ms/step - loss: 1.1598 - val_loss: 1.1790 - lr: 3.0000e-04\n",
      "Epoch 17/180\n",
      "500/500 [==============================] - 141s 283ms/step - loss: 1.1415 - val_loss: 1.1523 - lr: 3.0000e-04\n",
      "Epoch 18/180\n",
      "500/500 [==============================] - 139s 278ms/step - loss: 1.1262 - val_loss: 1.1814 - lr: 3.0000e-04\n",
      "Epoch 19/180\n",
      "500/500 [==============================] - 141s 282ms/step - loss: 1.1128 - val_loss: 1.1545 - lr: 3.0000e-04\n",
      "Epoch 20/180\n",
      "500/500 [==============================] - 142s 283ms/step - loss: 1.0984 - val_loss: 1.1431 - lr: 3.0000e-04\n",
      "Epoch 21/180\n",
      "500/500 [==============================] - 139s 279ms/step - loss: 1.0827 - val_loss: 1.1330 - lr: 3.0000e-04\n",
      "Epoch 22/180\n",
      "500/500 [==============================] - 141s 282ms/step - loss: 1.0690 - val_loss: 1.1155 - lr: 3.0000e-04\n",
      "Epoch 23/180\n",
      "500/500 [==============================] - 141s 282ms/step - loss: 1.0564 - val_loss: 1.1326 - lr: 3.0000e-04\n",
      "Epoch 24/180\n",
      "134/500 [=======>......................] - ETA: 1:34 - loss: 1.0432"
     ]
    }
   ],
   "source": [
    "# 记录每个 epoch 的学习率\n",
    "learning_rates = []\n",
    "\n",
    "dynamic_loss_weight_callback = DynamicLossWeight(cos_initial_weight=1.0, cos_final_weight=0.0, \n",
    "                                                huber_initial_weight=0.0, huber_final_weight=1.0, total_epochs = epochs)\n",
    "\n",
    "# 回调函数记录学习率\n",
    "class LearningRatePlotter(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        learning_rates.append(self.model.optimizer.lr.numpy())\n",
    "\n",
    "# 创建学习率回调\n",
    "lr_plotter = LearningRatePlotter()\n",
    "\n",
    "# 实例化动态权重 Callback\n",
    "\n",
    "# 训练模型时使用学习率调度器 lr_scheduler\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping,lr_scheduler, lr_plotter]  # 添加学习率调度器回调 [early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d364ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二阶段训练\n",
    "# for layer in model.layers[:13]:  # 假设冻结前22层,一共23层\n",
    "#     layer.trainable = False\n",
    "    \n",
    "# model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003))\n",
    "# history = model.fit(\n",
    "#     train_generator,\n",
    "#     epochs=epochs,\n",
    "#     steps_per_epoch=steps_per_epoch,\n",
    "#     validation_data=val_generator,\n",
    "#     validation_steps=validation_steps,\n",
    "#     callbacks=[early_stopping, lr_scheduler,lr_plotter]  # 添加学习率调度器回调 [early_stopping, lr_scheduler]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3660c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化学习率曲线\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(learning_rates)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d7c34-9013-4fe1-902b-cb13924bca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制验证损失和训练损失\n",
    "plt.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置字体大小\n",
    "plt.rcParams['font.size'] = 12\n",
    "# 设置图像大小\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 绘制训练和验证损失\n",
    "plt.plot(history.history['loss'], label='训练损失')\n",
    "plt.plot(history.history['val_loss'], label='验证损失')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('损失')\n",
    "plt.title('原始模型 训练和验证损失')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d1676-dd80-43c3-9ae9-dea85884889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save('model_gai10', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c62ba1-e5e2-42fa-8f8c-615979bc48f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
