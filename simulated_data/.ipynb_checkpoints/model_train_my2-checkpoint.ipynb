{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3367e635-b5d7-42d1-8c0d-170f23c6c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, multiply, Input, Dropout, Conv1D, Flatten, GRU\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Add, concatenate, GaussianNoise, Layer, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.activations import relu, elu\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from esinet import util\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from esinet import Simulation, Net\n",
    "from esinet.forward import create_forward_model, get_info\n",
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "007042cc-fba3-4f35-8298-196dd663375d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 1 0 1 0 0 1]\n",
      "[0.09 1.   0.25 0.   0.   0.   0.   0.   0.   0.  ]\n",
      "tf.Tensor(0.067, shape=(), dtype=float32)\n",
      "[0.09 0.   0.25 0.   0.   0.   0.   0.   0.   0.  ]\n",
      "[0.09 0.   0.25 0.   0.   0.   0.   0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "# 假设 y_true 是一个已经存在的数组\n",
    "y_pred = np.array([0.8, 1, 2.8, 0, 5, 0, -1.2, 0, 0, 0.1])\n",
    "y_true = np.array([0.5, 0, 2.3, 0, 5, 0, -1.2, 0, 0, 0.1])\n",
    "\n",
    "# 使用 numpy 的 where 函数创建二进制数组\n",
    "binary_array = np.where(y_true != 0, 1, 0)\n",
    "\n",
    "huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "\n",
    "basic_loss = (y_true - y_pred) ** 2\n",
    "\n",
    "vad_weighted_loss = basic_loss * binary_array  # 只对有语音的部分计算损失\n",
    "\n",
    "y_pred2=binary_array*y_pred\n",
    "y_true2=binary_array*y_true\n",
    "basic_loss2 = (y_true2 - y_pred2) ** 2\n",
    "\n",
    "# 显示结果\n",
    "print(binary_array)\n",
    "print(basic_loss)\n",
    "print(huber)\n",
    "print(vad_weighted_loss)\n",
    "print(basic_loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85344297-e008-410e-a19e-4e0f3a8aa4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Add()([x, inputs])\n",
    "\n",
    "    # Feed Forward Part\n",
    "    ff = LayerNormalization(epsilon=1e-6)(x)\n",
    "    ff = TimeDistributed(Dense(ff_dim, activation=\"relu\"))(ff)\n",
    "    ff = Dropout(dropout)(ff)\n",
    "    ff = TimeDistributed(Dense(inputs.shape[-1]))(ff)\n",
    "    ff = Add()([ff, x])\n",
    "\n",
    "    return ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b23856cb-f2fc-4148-9c1c-e8dc38ababf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "def myself_loss(y_true, y_pred):\n",
    "\tlosses = y_true - y_pred\n",
    "\treturn losses\n",
    "\n",
    "def non_zero_mse_loss(y_true, y_pred):\n",
    "\n",
    "    # 创建一个掩码，标记非零值\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    \n",
    "    # 使用掩码过滤出非零元素\n",
    "    y_true_filtered = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_filtered = tf.boolean_mask(y_pred, mask)\n",
    "    \n",
    "    # 计算非零元素的均方误差\n",
    "    # mse = tf.square(y_true_filtered - y_pred_filtered)\n",
    "    # mse = K.mean(K.square(y_true_filtered - y_pred_filtered))\n",
    "    mse = K.mean(K.square(y_true - y_pred))\n",
    "\n",
    "    print(\"The shape of pred is\", y_pred.shape)\n",
    "    \n",
    "    # 返回均方误差的平均值\n",
    "    return tf.reduce_mean(mse)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "\n",
    "    # MSE Loss\n",
    "    huber = tf.keras.losses.Huber(delta=0.1)(y_true, y_pred)\n",
    "    \n",
    "    # Cosine Similarity Loss\n",
    "    # 使用 tf.keras.losses.cosine_similarity，并确保结果为正值\n",
    "    cosine_loss = 1+tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "\n",
    "    l1_loss = tf.reduce_mean(tf.abs(y_pred))\n",
    "    \n",
    "    # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "    combined = 10000 * huber + 10*cosine_loss\n",
    "    return combined\n",
    "\n",
    "def combined_loss_transform(V):\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # MSE Loss\n",
    "        huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "        \n",
    "        # Cosine Similarity Loss\n",
    "        # 使用 tf.keras.losses.cosine_similarity，并确保结果为正值\n",
    "        cosine_loss = 1+tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    \n",
    "        transformed = tf.matmul(y_pred, V)\n",
    "        l1_loss = tf.reduce_mean(tf.abs(transformed))\n",
    "        \n",
    "        # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "        combined = 10000 * huber + 10*cosine_loss + 100*l1_loss\n",
    "        \n",
    "        return combined\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def combined_loss_Laplactransform(L):\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # MSE Loss\n",
    "        huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "        \n",
    "        # Cosine Similarity Loss\n",
    "        # 使用 tf.keras.losses.cosine_similarity，并确保结果为正值\n",
    "        cosine_loss = 1+tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    \n",
    "        # 计算拉普拉斯正则化项\n",
    "        # y_pred需要与拉普拉斯矩阵的维度匹配\n",
    "        laplacian_reg = tf.matmul(tf.matmul(y_pred, L), y_pred, transpose_a=True)\n",
    "        l1_loss = tf.reduce_mean(tf.abs(laplacian_reg))\n",
    "        \n",
    "        # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "        combined = 10000 * huber + 3*cosine_loss + 100*l1_loss\n",
    "        \n",
    "        return combined\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def wight_combined_loss(y_true, y_pred):\n",
    "    \n",
    "    # # 设定权重\n",
    "    # binary_array=tf.where(y_true != 0, 1, 0)\n",
    "    # binary_array = tf.cast(binary_array, dtype=tf.float32)\n",
    "    # y_true_filter = tf.multiply(binary_array, y_true)\n",
    "    # y_pred_filter = tf.multiply(binary_array, y_pred)\n",
    "\n",
    "    # 创建一个掩码，标记非零值\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    # 使用掩码过滤出非零元素\n",
    "    y_true_filtered = tf.boolean_mask(y_true, mask)\n",
    "    y_pred_filtered = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "    # MSE Loss\n",
    "    huber = tf.keras.losses.Huber(delta=0.1)(y_true, y_pred)\n",
    "    # huber2 = tf.keras.losses.Huber(delta=0.1)(y_true_filter, y_pred_filter)\n",
    "    huber2 = tf.keras.losses.MeanSquaredError()(y_true_filtered, y_pred_filtered)\n",
    "    \n",
    "    # Cosine Similarity Loss\n",
    "    # 使用 tf.keras.losses.cosine_similarity，并确保结果为正值\n",
    "    cosine_loss = 1+tf.keras.losses.CosineSimilarity()(y_true, y_pred)\n",
    "    \n",
    "    # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "    # combined = 1000 * huber + 1*cosine_loss + 100 * huber2\n",
    "    combined = 1000 * huber + 1*cosine_loss + huber2\n",
    "\n",
    "    return combined\n",
    "    \n",
    "def sparsity(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred)) / K.max(K.square(y_pred))\n",
    "def combined_loss2(y_true, y_pred):\n",
    "\n",
    "    print(y_pred.shape)\n",
    "    print(y_true.shape)\n",
    "    # MSE Loss\n",
    "    huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "\n",
    "    sparsity_loss = sparsity(y_true, y_pred)\n",
    "\n",
    "    print(sparsity_loss)\n",
    "    \n",
    "    # 组合损失，确保使用 tf.cast 保持类型一致\n",
    "    combined = huber + 0.1 * sparsity_loss\n",
    "    return combined\n",
    "\n",
    "def elastic_net_loss(alpha):\n",
    "    def loss(y_true, y_pred):\n",
    "        huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "       # l1_loss = K.sum(K.abs(y_pred))\n",
    "        l1_loss = tf.reduce_mean(tf.abs(y_pred))\n",
    "        total_loss = 10000*huber + alpha * l1_loss \n",
    "        return total_loss\n",
    "    return loss\n",
    "\n",
    "def combined_loss3(leadfield):\n",
    "    def loss(y_true, y_pred):\n",
    "        huber = tf.keras.losses.Huber()(y_true, y_pred)\n",
    "        y_est = np.squeeze(np.array(y_pred))\n",
    "        x_true = np.squeeze(np.array(x_true))\n",
    "        # Get EEG from predicted source using leadfield\n",
    "        x_est = np.matmul(leadfield, y_est)\n",
    "        total_loss = huber + alpha * l1_loss \n",
    "        return total_loss\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca07262f-68d0-4889-9a41-d4102e59e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelRegionDropout(Layer):\n",
    "    def __init__(self, block_size=3, **kwargs):\n",
    "        super(ChannelRegionDropout, self).__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training is None:\n",
    "            training = K.learning_phase()\n",
    "\n",
    "        def dropped_inputs():\n",
    "            input_shape = tf.shape(inputs)\n",
    "            batch_size = input_shape[0]\n",
    "            timesteps = input_shape[1]\n",
    "            channels = input_shape[2]\n",
    "\n",
    "            # 计算需要生成的掩码大小\n",
    "            mask_size = channels - self.block_size + 1\n",
    "\n",
    "            # 随机选择一个起始点\n",
    "            start_channel = tf.random.uniform(shape=(), minval=0, maxval=mask_size, dtype=tf.int32)\n",
    "\n",
    "            # 生成基础掩码\n",
    "            mask = tf.concat([\n",
    "                tf.ones((batch_size, timesteps, start_channel), dtype=inputs.dtype),\n",
    "                tf.zeros((batch_size, timesteps, self.block_size), dtype=inputs.dtype),\n",
    "                tf.ones((batch_size, timesteps, channels - start_channel - self.block_size), dtype=inputs.dtype)\n",
    "            ], axis=2)\n",
    "\n",
    "            return inputs * mask\n",
    "\n",
    "        return K.in_train_phase(dropped_inputs(), inputs, training=training)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2320d27-600c-48a0-99d0-65c838a51356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelDropout(Layer):\n",
    "    def __init__(self, drop_rate, block_size, **kwargs):\n",
    "        super(ChannelDropout, self).__init__(**kwargs)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ChannelDropout, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            input_shape = tf.shape(inputs)\n",
    "            batch_size = input_shape[0]\n",
    "            timesteps = input_shape[1]\n",
    "            channels = input_shape[2]\n",
    "\n",
    "            # 计算缩减后的通道数\n",
    "            reduced_channels = (channels + self.block_size - 1) // self.block_size\n",
    "\n",
    "            # 生成基本掩码\n",
    "            random_tensor = tf.random.uniform((batch_size, timesteps, reduced_channels), dtype=inputs.dtype)\n",
    "            dropout_mask = tf.cast(random_tensor < (1 - self.drop_rate), inputs.dtype)\n",
    "\n",
    "            # 扩展掩码以覆盖原始维度\n",
    "            dropout_mask = tf.tile(dropout_mask, [1, 1, self.block_size])\n",
    "            # 裁剪掩码以匹配输入尺寸\n",
    "            dropout_mask = tf.slice(dropout_mask, [0, 0, 0], [batch_size, timesteps, channels])\n",
    "\n",
    "            # 应用掩码\n",
    "            return inputs * dropout_mask\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c66820ce-1f68-4b6a-98c7-5ab6de58263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropout(Layer):\n",
    "    def __init__(self, drop_rate, block_size_time, block_size_channel, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.block_size_time = block_size_time\n",
    "        self.block_size_channel = block_size_channel\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(CustomDropout, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            input_shape = tf.shape(inputs)\n",
    "            batch_size = input_shape[0]\n",
    "            timesteps = input_shape[1]\n",
    "            channels = input_shape[2]\n",
    "\n",
    "            # 计算缩减后的时间步和通道数\n",
    "            reduced_timesteps = (timesteps + self.block_size_time - 1) // self.block_size_time\n",
    "            reduced_channels = (channels + self.block_size_channel - 1) // self.block_size_channel\n",
    "\n",
    "            # 生成基本掩码\n",
    "            random_tensor = tf.random.uniform((batch_size, reduced_timesteps, reduced_channels), dtype=inputs.dtype)\n",
    "            dropout_mask = tf.cast(random_tensor < (1 - self.drop_rate), inputs.dtype)\n",
    "\n",
    "            # 扩展掩码以覆盖原始维度\n",
    "            dropout_mask = tf.tile(dropout_mask, [1, self.block_size_time, self.block_size_channel])\n",
    "            # 裁剪掩码以匹配输入尺寸\n",
    "            dropout_mask = tf.slice(dropout_mask, [0, 0, 0], [batch_size, timesteps, channels])\n",
    "\n",
    "            # 应用掩码\n",
    "            return inputs * dropout_mask\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b9dcd11-d654-49ae-a7a6-9aa36c4a2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateSpatialDropout1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, start_step=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.rate = rate\n",
    "        self.start_step = start_step\n",
    "        self.spatial_dropout = tf.keras.layers.SpatialDropout1D(rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = tf.cond(self._train_counter < self.start_step,\n",
    "                    lambda: inputs,\n",
    "                    lambda: self.spatial_dropout(inputs, training=training))\n",
    "\n",
    "        if training:\n",
    "            self._train_counter.assign_add(1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fde9ac92-78d9-45d0-a7f7-e69d14451a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对输入eeg进行归一化\n",
    "def custom_prep_data(data):\n",
    "\n",
    "    data = np.swapaxes(data, 1,2)\n",
    "\n",
    "    # 获取数据维度\n",
    "    num_samples, num_timepoints, num_channels = data.shape\n",
    "    \n",
    "    # 将数据类型转换为 np.float32\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    # 对每个样本（脑电信号的一个时间点）进行处理\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_timepoints):\n",
    "            # 获取当前样本的数据\n",
    "            sample_data = data[i, j, :]\n",
    "            \n",
    "            # 假设你想要进行去除平均值和标准化的预处理\n",
    "            # 去除平均值\n",
    "            sample_data_mean = np.mean(sample_data)\n",
    "            sample_data_std = np.std(sample_data)\n",
    "            sample_data -= sample_data_mean\n",
    "            \n",
    "            # 标准化\n",
    "            if sample_data_std != 0:\n",
    "                sample_data /= sample_data_std\n",
    "        \n",
    "            # 更新数据\n",
    "            data[i, j, :] = sample_data\n",
    "    print(\"The shape of EEG is\", data.shape)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 对源信号进行归一化\n",
    "def custom_prep_source(data):\n",
    "    \n",
    "    # 将数据类型转换为 np.float32\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    for i, y_sample in enumerate(data):\n",
    "        data[i] /= np.max(abs(data[i]))\n",
    "\n",
    "    data = np.swapaxes(data, 1,2)\n",
    "    print(\"The shape of source is\", data.shape)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f287b6a0-fe37-433e-986c-815976091200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义学习率调度函数\n",
    "def lr_schedule(epoch):\n",
    "    # 根据训练周期(epoch)来动态调整学习率\n",
    "    if epoch < 50:\n",
    "        return 0.0005  # 0.0005\n",
    "    elif epoch < 100:\n",
    "        return 0.0003  # 0.0003\n",
    "    elif epoch < 150:\n",
    "        return 0.0001  # 0.0002\n",
    "    else:\n",
    "        return 0.00005  # 0.0001\n",
    "\n",
    "# 创建学习率调度器,损失函数有余弦相似度 CosineSimilarity, tf.keras.losses.Huber(), MeanAbsoluteError, MeanSquaredError\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7dea1584-d54a-43a4-93e1-343f9644802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simplified_Hybrid_Model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, None, 60)]   0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, None, 60)    120         ['Input[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, None, 60)    124476      ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, None, 60)     0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'Input[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, None, 60)    120         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDistri  (None, None, 512)   31232       ['layer_normalization_5[0][0]']  \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, None, 512)    0           ['time_distributed_4[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDistri  (None, None, 60)    30780       ['dropout_2[0][0]']              \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, None, 60)     0           ['time_distributed_5[0][0]',     \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " Direct (TimeDistributed)       (None, None, 200)    12200       ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " Mask (TimeDistributed)         (None, None, 200)    12200       ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " BiLSTM1 (Bidirectional)        (None, None, 200)    128800      ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, None, 200)    0           ['Direct[0][0]',                 \n",
      "                                                                  'Mask[0][0]']                   \n",
      "                                                                                                  \n",
      " BiLSTM2 (Bidirectional)        (None, None, 200)    240800      ['BiLSTM1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, None, 400)    0           ['multiply[0][0]',               \n",
      "                                                                  'BiLSTM2[0][0]']                \n",
      "                                                                                                  \n",
      " BiLSTM3 (Bidirectional)        (None, None, 500)    1302000     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " Direct_Output (TimeDistributed  (None, None, 1284)  643284      ['BiLSTM3[0][0]']                \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,526,012\n",
      "Trainable params: 2,526,012\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 超参数定义, kernel_initializer='he_uniform', kernel_initializer='glorot_uniform' dropout=dropout_rate\n",
    "n_channels = 60\n",
    "n_dipoles = 1284\n",
    "n_dense_units = 200  # 减少单元数 200\n",
    "n_lstm_units1 = 100    # 减少单元数 100\n",
    "n_lstm_units2 = 150    # 减少单元数 100\n",
    "n_lstm_units3 = 250    # 减少单元数 100\n",
    "dropout_rate = 0.1 # 0.2\n",
    "batch_size = 32 # 32 2048 2的次幂\n",
    "epochs = 200\n",
    "noise_factor=0.1\n",
    "V=np.load('D:/jupyter_note/SWX_source/Simulated_data/V.npy')\n",
    "L=np.load('D:/jupyter_note/SWX_source/Simulated_data/L.npy')\n",
    "\n",
    "# 输入层\n",
    "inputs = Input(shape=(None, n_channels), name='Input')\n",
    "# noisy_input = GaussianNoise(stddev=noise_factor)(inputs, training=True)\n",
    "\n",
    "# 并行处理输入\n",
    "transformer_block = transformer_encoder(inputs, head_size=128, num_heads=4, ff_dim=512)\n",
    "direct = TimeDistributed(Dense(n_dense_units, activation=\"linear\"), name='Direct')(transformer_block)\n",
    "mask = TimeDistributed(Dense(n_dense_units, activation=\"sigmoid\"), name='Mask')(transformer_block)\n",
    "multi = multiply([direct, mask], name=\"multiply\")\n",
    "bi_lstm = Bidirectional(LSTM(n_lstm_units1, return_sequences=True, dropout=dropout_rate), name='BiLSTM1')(inputs)\n",
    "bi_lstm = Bidirectional(LSTM(n_lstm_units1, return_sequences=True, dropout=dropout_rate), name='BiLSTM2')(bi_lstm)\n",
    "\n",
    "\n",
    "# 合并并行层的输出\n",
    "merged = concatenate([multi, bi_lstm], axis=-1)\n",
    "\n",
    "# LSTM层\n",
    "lstm = Bidirectional(LSTM(n_lstm_units3, return_sequences=True, dropout=dropout_rate), name='BiLSTM3')(merged)\n",
    "\n",
    "# 输出层\n",
    "direct_out = TimeDistributed(Dense(n_dipoles, activation=\"tanh\"), name='Direct_Output')(lstm)\n",
    "\n",
    "# 创建和编译模型\n",
    "model = tf.keras.Model(inputs=inputs, outputs=direct_out, name='Simplified_Hybrid_Model')\n",
    "model.compile(loss=combined_loss, optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))\n",
    "\n",
    "# 打印模型概要\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f45b428-7827-45be-b1c8-2658493a6128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 60, 26)\n",
      "(20000, 1284, 26)\n"
     ]
    }
   ],
   "source": [
    "# 加载测试数据\n",
    "x= np.load('D:/jupyter_note/SWX_source/Simulated_data/x3.npy')\n",
    "y= np.load('D:/jupyter_note/SWX_source/Simulated_data/y3.npy')\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1cb47b7-8abf-4c92-9934-87a5125b3e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of EEG is (20000, 26, 60)\n",
      "The shape of source is (20000, 26, 1284)\n"
     ]
    }
   ],
   "source": [
    " # 对信号进行预处理\n",
    "x = custom_prep_data(x)\n",
    "y = custom_prep_source(y)\n",
    "# y = np.swapaxes(y, 1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10311a34-042b-4c73-91d7-c7b88295f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用数据生成器进行训练\n",
    "def data_generator(x, y, batch_size):\n",
    "    num_samples = len(x)\n",
    "    indices = np.arange(num_samples)\n",
    "    \n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            yield x[batch_indices], y[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3d9dc45c-bb55-4420-a472-15c125689bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "split_index = int(0.9 * len(x))\n",
    "x_train, x_val = x[:split_index], x[split_index:]\n",
    "y_train, y_val = y[:split_index], y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "361c1691-4363-4f67-94e6-b98e1abd9a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators for training and validation\n",
    "train_generator = data_generator(x_train, y_train, batch_size)\n",
    "val_generator = data_generator(x_val, y_val, batch_size)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=35, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34a36e7c-5b6b-4b09-ba24-d01f529a64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算每个epoch的步骤和验证步骤\n",
    "steps_per_epoch = len(x_train) // batch_size\n",
    "validation_steps = len(x_val) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa333345-1fbc-4855-b857-b93b95f5649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "562/562 [==============================] - 32s 37ms/step - loss: 11.7601 - val_loss: 10.2046 - lr: 5.0000e-04\n",
      "Epoch 2/200\n",
      "562/562 [==============================] - 18s 33ms/step - loss: 10.0283 - val_loss: 9.5826 - lr: 5.0000e-04\n",
      "Epoch 3/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 9.2502 - val_loss: 8.6337 - lr: 5.0000e-04\n",
      "Epoch 4/200\n",
      "562/562 [==============================] - 19s 33ms/step - loss: 8.3880 - val_loss: 7.9375 - lr: 5.0000e-04\n",
      "Epoch 5/200\n",
      "562/562 [==============================] - 19s 34ms/step - loss: 7.8364 - val_loss: 7.5209 - lr: 5.0000e-04\n",
      "Epoch 6/200\n",
      "562/562 [==============================] - 19s 34ms/step - loss: 7.4389 - val_loss: 7.2087 - lr: 5.0000e-04\n",
      "Epoch 7/200\n",
      "562/562 [==============================] - 19s 34ms/step - loss: 7.1443 - val_loss: 6.9999 - lr: 5.0000e-04\n",
      "Epoch 8/200\n",
      "562/562 [==============================] - 19s 33ms/step - loss: 6.9126 - val_loss: 6.8720 - lr: 5.0000e-04\n",
      "Epoch 9/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 6.7226 - val_loss: 6.7082 - lr: 5.0000e-04\n",
      "Epoch 10/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 6.5556 - val_loss: 6.5854 - lr: 5.0000e-04\n",
      "Epoch 11/200\n",
      "562/562 [==============================] - 18s 33ms/step - loss: 6.4084 - val_loss: 6.4888 - lr: 5.0000e-04\n",
      "Epoch 12/200\n",
      "562/562 [==============================] - 18s 33ms/step - loss: 6.2672 - val_loss: 6.3656 - lr: 5.0000e-04\n",
      "Epoch 13/200\n",
      "562/562 [==============================] - 19s 33ms/step - loss: 6.1546 - val_loss: 6.3462 - lr: 5.0000e-04\n",
      "Epoch 14/200\n",
      "562/562 [==============================] - 19s 33ms/step - loss: 6.0474 - val_loss: 6.2958 - lr: 5.0000e-04\n",
      "Epoch 15/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 5.9579 - val_loss: 6.2088 - lr: 5.0000e-04\n",
      "Epoch 16/200\n",
      "562/562 [==============================] - 19s 34ms/step - loss: 5.8680 - val_loss: 6.2389 - lr: 5.0000e-04\n",
      "Epoch 17/200\n",
      "562/562 [==============================] - 18s 33ms/step - loss: 5.7840 - val_loss: 6.1479 - lr: 5.0000e-04\n",
      "Epoch 18/200\n",
      "562/562 [==============================] - 18s 33ms/step - loss: 5.7035 - val_loss: 6.0948 - lr: 5.0000e-04\n",
      "Epoch 19/200\n",
      "562/562 [==============================] - 19s 34ms/step - loss: 5.6365 - val_loss: 6.0594 - lr: 5.0000e-04\n",
      "Epoch 20/200\n",
      "562/562 [==============================] - 18s 33ms/step - loss: 5.5587 - val_loss: 6.0105 - lr: 5.0000e-04\n",
      "Epoch 21/200\n",
      "562/562 [==============================] - 18s 33ms/step - loss: 5.5073 - val_loss: 5.9625 - lr: 5.0000e-04\n",
      "Epoch 22/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 5.4330 - val_loss: 5.9557 - lr: 5.0000e-04\n",
      "Epoch 23/200\n",
      "562/562 [==============================] - 17s 31ms/step - loss: 5.3887 - val_loss: 5.9702 - lr: 5.0000e-04\n",
      "Epoch 24/200\n",
      "562/562 [==============================] - 17s 31ms/step - loss: 5.3272 - val_loss: 5.8952 - lr: 5.0000e-04\n",
      "Epoch 25/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 5.2798 - val_loss: 5.9360 - lr: 5.0000e-04\n",
      "Epoch 26/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 5.2341 - val_loss: 5.8330 - lr: 5.0000e-04\n",
      "Epoch 27/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 5.1753 - val_loss: 5.8635 - lr: 5.0000e-04\n",
      "Epoch 28/200\n",
      "562/562 [==============================] - 17s 31ms/step - loss: 5.1402 - val_loss: 5.8956 - lr: 5.0000e-04\n",
      "Epoch 29/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 5.0996 - val_loss: 5.8588 - lr: 5.0000e-04\n",
      "Epoch 30/200\n",
      "562/562 [==============================] - 18s 31ms/step - loss: 5.0535 - val_loss: 5.8189 - lr: 5.0000e-04\n",
      "Epoch 31/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 5.0128 - val_loss: 5.7865 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.9813 - val_loss: 5.7869 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "562/562 [==============================] - 17s 31ms/step - loss: 4.9444 - val_loss: 5.7392 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.9058 - val_loss: 5.7447 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.8780 - val_loss: 5.7435 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.8389 - val_loss: 5.7337 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "562/562 [==============================] - 17s 31ms/step - loss: 4.8143 - val_loss: 5.7198 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.7817 - val_loss: 5.7031 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.7616 - val_loss: 5.7150 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "562/562 [==============================] - 17s 31ms/step - loss: 4.7273 - val_loss: 5.6339 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.6998 - val_loss: 5.6645 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "562/562 [==============================] - 18s 31ms/step - loss: 4.6722 - val_loss: 5.6696 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.6497 - val_loss: 5.6509 - lr: 5.0000e-04\n",
      "Epoch 44/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.6205 - val_loss: 5.6524 - lr: 5.0000e-04\n",
      "Epoch 45/200\n",
      "562/562 [==============================] - 18s 31ms/step - loss: 4.6191 - val_loss: 5.6600 - lr: 5.0000e-04\n",
      "Epoch 46/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.5807 - val_loss: 5.6368 - lr: 5.0000e-04\n",
      "Epoch 47/200\n",
      "562/562 [==============================] - 18s 31ms/step - loss: 4.5623 - val_loss: 5.6195 - lr: 5.0000e-04\n",
      "Epoch 48/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.5324 - val_loss: 5.6126 - lr: 5.0000e-04\n",
      "Epoch 49/200\n",
      "562/562 [==============================] - 17s 31ms/step - loss: 4.5284 - val_loss: 5.6090 - lr: 5.0000e-04\n",
      "Epoch 50/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.4881 - val_loss: 5.5947 - lr: 5.0000e-04\n",
      "Epoch 51/200\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 4.3314 - val_loss: 5.4876 - lr: 3.0000e-04\n",
      "Epoch 52/200\n",
      "562/562 [==============================] - 18s 31ms/step - loss: 4.3087 - val_loss: 5.4846 - lr: 3.0000e-04\n",
      "Epoch 53/200\n",
      "562/562 [==============================] - 18s 31ms/step - loss: 4.2910 - val_loss: 5.4695 - lr: 3.0000e-04\n",
      "Epoch 54/200\n",
      "497/562 [=========================>....] - ETA: 1s - loss: 4.2682"
     ]
    }
   ],
   "source": [
    "# 训练模型时使用学习率调度器\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, lr_scheduler]  # 添加学习率调度器回调\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d7c34-9013-4fe1-902b-cb13924bca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制验证损失和训练损失\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置字体大小\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# 设置图像大小\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 绘制训练和验证损失\n",
    "plt.plot(history.history['loss'], label='训练损失')\n",
    "plt.plot(history.history['val_loss'], label='验证损失')\n",
    "plt.xlabel('迭代次数')\n",
    "plt.ylabel('损失')\n",
    "plt.title('原始模型 训练和验证损失')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d1676-dd80-43c3-9ae9-dea85884889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "model.save('D:/jupyter_note/SWX_source/Simulated_data/model3', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a17f64-b959-476f-8487-678742157ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Keras]",
   "language": "python",
   "name": "conda-env-Keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
